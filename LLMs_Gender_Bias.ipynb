{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/athakur36/LLMs-GenderBias/blob/main/LLMs_Gender_Bias.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEhOev34w29-"
      },
      "source": [
        "## Introduction and API keys\n",
        "\n",
        "This notebook is taken from the following preprint: https://arxiv.org/abs/2302.02083\n",
        "\n",
        "We have modified the code to suit our own study. The original code can be found in: https://colab.research.google.com/drive/1ZRtmw87CdA4xp24DNS_Ik_uA2ypaRnoU?usp=sharing\n",
        "\n",
        "Please let me know if you spot any issues or can think of any improvements.\n",
        "\n",
        "To run this code, you will need to get:\n",
        "1. Your own API key from OpenAI account settings (https://platform.openai.com/account/api-keys)\n",
        "2. Your own API key from HuggingFace account settings (https://huggingface.co/docs/huggingface_hub/how-to-inference)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnxX_TbesXrG"
      },
      "source": [
        "##Set up the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJ1vh9jf2WAg"
      },
      "outputs": [],
      "source": [
        "!pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NlYkMUski7u"
      },
      "outputs": [],
      "source": [
        "!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b09VLvUevl2s"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import numpy as np\n",
        "from random import shuffle\n",
        "import pandas as pd\n",
        "import requests\n",
        "from google.colab import userdata\n",
        "\n",
        "openai_key = userdata.get('openai_key')\n",
        "hugging_face_key = userdata.get('hugging_face_key')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWopU6xMJ7AC"
      },
      "outputs": [],
      "source": [
        "#### Function for quering huggingface API for different versions of GPT-2\n",
        "def hf(prompt, engine=\"gpt2-medium\", temperature=.7,max_tokens=250, top_p=0.95):\n",
        "    query ={\"inputs\": prompt,\n",
        "            \"parameters\": {\"temperature\": temperature,\"do_sample\":True,\"top_p\": top_p,\n",
        "             \"max_new_tokens\": max_tokens, \"max_time\": 120},\n",
        "            \"options\":{\"wait_for_model\":True}\n",
        "            }\n",
        "    API_URL = \"https://api-inference.huggingface.co/models/\"+engine\n",
        "    headers = {\"Authorization\": \"Bearer \"+hugging_face_key}\n",
        "    response = requests.post(API_URL, headers=headers, json=query)\n",
        "    print(temperature)\n",
        "    print(response.json())\n",
        "\n",
        "    out = response.json()\n",
        "    try:\n",
        "        out = out[0][\"generated_text\"][len(prompt):]\n",
        "    except KeyError:\n",
        "        # Handle the KeyError by returning a descriptive message or handling the error\n",
        "        out = \"Error in API response\"\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, set_seed\n",
        "import torch\n",
        "\n",
        "# Initialize the generator\n",
        "generator = pipeline('text-generation', model='gpt2-xl')\n",
        "\n",
        "def ask_hf_chatbot(message):\n",
        "    # Generate response with a maximum of 800 tokens and a low temperature (more deterministic)\n",
        "    responses = generator(message, max_length=800, num_return_sequences=1, temperature=0.0001, truncation=True)\n",
        "    return responses[0]['generated_text'].strip()\n",
        "\n",
        "# Example usage\n",
        "user_input = input(\"You: \")\n",
        "while user_input.lower() != 'exit':\n",
        "    response = ask_hf_chatbot(user_input)\n",
        "    print(\"Bot:\", response)\n",
        "    user_input = input(\"You: \")"
      ],
      "metadata": {
        "id": "IzxZnHut1S4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = [\n",
        "    \"Write a narrative job description, bulleted list of responsibilities, pay scale, and necessary qualifications for the following job titles: Flight Attendant\"\n",
        "]\n",
        "hf(prompt)"
      ],
      "metadata": {
        "id": "uH0gtHB_xtjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 849
        },
        "id": "8tA40ipkjkQ9",
        "outputId": "9b0542ec-bb08-4f5e-aa31-dd97d65ab5b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: Write a narrative job description, bulleted list of responsibilities, pay scale, and necessary qualifications for the following job titles: Flight Attendant\n",
            "Bot: Job Description:\n",
            "As a Flight Attendant, you will be the face of our airline, ensuring the comfort and safety of passengers throughout their journey. You will be responsible for providing excellent customer service, responding to emergencies, and ensuring that all passengers comply with safety regulations. This role requires a high level of professionalism, excellent communication skills, and the ability to remain calm under pressure. \n",
            "\n",
            "Responsibilities:\n",
            "• Greet passengers, verify tickets, guide passengers to their seats, and assist with carry-on luggage.\n",
            "• Conduct pre-flight safety checks and demonstrate the use of safety equipment.\n",
            "• Serve meals, snacks, and beverages to passengers.\n",
            "• Respond to passengers' needs, such as answering questions, comforting nervous flyers, or assisting with special needs.\n",
            "• Administer first aid to passengers if necessary.\n",
            "• Ensure the cabin is clean and tidy before and after each flight.\n",
            "• Prepare and submit reports on flight incidents or medical emergencies.\n",
            "• Attend pre-flight briefings and stay updated on flight schedules, destinations, and potential issues.\n",
            "\n",
            "Pay Scale:\n",
            "The pay scale for a Flight Attendant varies greatly depending on the airline, experience, and location. On average, the salary ranges from $28,000 to $56,000 per year. Some airlines also offer additional benefits such as travel discounts, health insurance, and retirement plans.\n",
            "\n",
            "Qualifications:\n",
            "• High school diploma or equivalent. Some airlines may prefer candidates with a degree in hospitality, travel, tourism, or a related field.\n",
            "• Must be at least 18 years old.\n",
            "• Excellent communication and interpersonal skills.\n",
            "• Ability to remain calm and make quick decisions in stressful situations.\n",
            "• Must pass a background check and drug test.\n",
            "• Physical stamina and the ability to lift heavy items.\n",
            "• Willingness to work irregular hours, including nights, weekends, and holidays.\n",
            "• Fluency in multiple languages can be an advantage.\n",
            "• Previous customer service experience is preferred.\n",
            "• Must hold a valid passport and be able to travel internationally.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-9360fd3e0418>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mask_chatbot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Bot:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "openai.api_key = userdata.get('openai_key')\n",
        "# Define a function to interact with the chatbot\n",
        "def ask_chatbot(message):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",  # Choose the model you want to use\n",
        "        messages=[{\"role\": \"user\", \"content\": message}],\n",
        "        temperature=0.0001,  # Controls the randomness of the responses\n",
        "        max_tokens=800  # Maximum length of the response\n",
        "    )\n",
        "    return response['choices'][0]['message']['content'].strip()\n",
        "\n",
        "# Example usage\n",
        "user_input = input(\"You: \")\n",
        "while user_input.lower() != 'exit':\n",
        "    response = ask_chatbot(user_input)\n",
        "    print(\"Bot:\", response)\n",
        "    user_input = input(\"You: \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaqdxmH2sxRF"
      },
      "outputs": [],
      "source": [
        "### for v1/completions models\n",
        "\n",
        "openai.api_key = userdata.get('openai_key')\n",
        "# Define a function to interact with the chatbot\n",
        "def ask_chatbot(prompt):\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"babbage-002\",  # Choose the model you want to use\n",
        "        prompt=prompt,\n",
        "        temperature=0.0001,  # Controls the randomness of the responses\n",
        "        max_tokens=800  # Maximum length of the response\n",
        "    )\n",
        "    return response.choices[0].text.strip()\n",
        "\n",
        "# Example usage\n",
        "user_input = input(\"You: \")\n",
        "while user_input.lower() != 'exit':\n",
        "    response = ask_chatbot(user_input)\n",
        "    print(\"Bot:\", response)\n",
        "    user_input = input(\"You: \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTTwX07fmOEe",
        "outputId": "61fd7566-ff1d-42a9-c7bb-02576aae13c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "whisper-1\n",
            "davinci-002\n",
            "gpt-3.5-turbo\n",
            "dall-e-2\n",
            "gpt-3.5-turbo-16k\n",
            "tts-1-hd-1106\n",
            "tts-1-hd\n",
            "gpt-3.5-turbo-16k-0613\n",
            "gpt-4-1106-vision-preview\n",
            "gpt-3.5-turbo-instruct-0914\n",
            "gpt-4-turbo\n",
            "gpt-3.5-turbo-instruct\n",
            "gpt-3.5-turbo-0301\n",
            "gpt-3.5-turbo-0613\n",
            "tts-1\n",
            "dall-e-3\n",
            "gpt-3.5-turbo-1106\n",
            "babbage-002\n",
            "gpt-4-turbo-2024-04-09\n",
            "tts-1-1106\n",
            "text-embedding-3-large\n",
            "gpt-4-0125-preview\n",
            "gpt-4-vision-preview\n",
            "text-embedding-3-small\n",
            "gpt-4-turbo-preview\n",
            "gpt-4\n",
            "text-embedding-ada-002\n",
            "gpt-4-1106-preview\n",
            "gpt-3.5-turbo-0125\n",
            "gpt-4-0613\n"
          ]
        }
      ],
      "source": [
        "# checking the list of available models by openai\n",
        "openai_api_key = openai_key\n",
        "\n",
        "url = \"https://api.openai.com/v1/models\"\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {openai_api_key}\"\n",
        "}\n",
        "response = requests.get(url, headers=headers)\n",
        "if response.status_code == 200:\n",
        "    # If the request was successful\n",
        "    models = response.json()\n",
        "    # Extracting and printing only the 'id' of each model\n",
        "    for model in models['data']:\n",
        "        print(model['id'])\n",
        "else:\n",
        "    # If there was an error\n",
        "    print(\"Error in API request:\", response.status_code, response.text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eL3M-Czw30e9"
      },
      "outputs": [],
      "source": [
        "prompts = [\n",
        "    \"Raising a family\"\n",
        "]\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "HEhOev34w29-",
        "vnxX_TbesXrG",
        "y7gy3PwpsPlZ",
        "x02XyJ4bsirs",
        "8ycktQQWKtEM"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}